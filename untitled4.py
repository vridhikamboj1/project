# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qFBw2kJQtbMS0Er5mztZKAY-tWuaF8VN
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df=pd.read_csv("housing.csv")

df

df.info()

df.describe()

"""the minimum value of the house is 105000
the maximum value of the price is 1024800
the average or the mean value of the price is 454342.9
"""





df.isnull().sum()

prices=df['MEDV']

prices

features=df.drop('MEDV',axis=1)

features

import seaborn as sns

sns.regplot('RM','MEDV',data=df)

"""medv value is directly correlated with the rm where rm is the avg no of rooms in the neighbourhood.higher the rm value means that its worth will be more.
so it is easy to say that higher the rm value higher is going to be medv value

"""

sns.regplot('LSTAT','MEDV',data=df)

"""lstat is the  neighbourhood with low class people 
so more the no lstat less is going to be the worth of the house
or less the medv value
latat is negative proportional to the medv value
"""

sns.regplot('PTRATIO','MEDV',data=df)

"""- Neighborhoods with more students to teachers ratio (higher 'PTRATIO' value) will be worth less. If the percentage of students to teachers ratio people is higher, it is likely that in the neighborhood there are less schools, this could be because there is less taxes income which could be because in that neighborhood people earn less money. If people earn less money it is likely that their houses are worth less. They are inversely proportional variables.

"""

sns.pairplot(df)

from sklearn.metrics import r2_score
def performance_metric(y_true,y_predict):
  score=r2_score(y_true,y_pred)
  return score

"""importing the r2 score"""

x=df.drop('MEDV',axis=1)
y=df.MEDV

from sklearn.linear_model import LinearRegression

scores=[]

lr=LinearRegression()







from sklearn.model_selection import KFold,cross_val_predict

kf=KFold(shuffle=True,random_state=72018,n_splits=3)
for  train_index,test_index in kf.split(x):
  print('train_index:',train_index[:10],len(train_index))
  print('test_index:',test_index[:10],len(test_index))
  print('')

for train_index,test_index in kf.split(x):
  x_train,x_test,y_train,y_test=(x.iloc[train_index,:],x.iloc[test_index,:],y[train_index],y[test_index])
  lr.fit(x_train,y_train)
  y_pred=lr.predict(x_test)
  score=r2_score(y_test.values,y_pred)
  scores.append(score)
  scores

scores

kf

from sklearn.pipeline import Pipeline

from sklearn.preprocessing import StandardScaler

a=StandardScaler()

estimator=Pipeline([ ('scaler',a),('regression',lr)])

predictions=cross_val_predict(estimator,x,y,cv=kf)

len(predictions)

r2_score(y,predictions)

np.mean(scores)

from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso

alphas=np.geomspace(1e-9,1e0,num=10)

alphas

coef=[]
for alpha in alphas:
  las=Lasso(alpha=alpha,max_iter=10000)
  estimator=Pipeline([('scalar',a),('lasso_regression',las)])

predictions=cross_val_predict(estimator,x,y,cv=kf)

score=r2_score(y,predictions)

score

coef=[]
for alpha in alphas:
  ridge=Ridge(alpha=alpha,max_iter=10000)
  estimator=Pipeline([('scalar',a),('ridge',ridge)])

predictions=cross_val_predict(estimator,x,y,cv=kf)
score=r2_score(y,predictions)

score

lr=LinearRegression()

lr.fit(x_train,y_train)

estimator=Pipeline([('scalar',a),('linear',lr)])

predictions=cross_val_predict(estimator,x,y,cv=kf)

score=r2_score(y,predictions)

score